{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/spark-storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare the primitive RDD with Modern DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory', '12g')\n",
    "conf.set('spark.executor.memory', '1g')\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.1')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# Add keys\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3Key)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3Secret)\n",
    "hadoopConf.setInt(\"fs.s3a.connection.maximum\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data = sc.textFile(\"<csv path>\")\n",
    "rdd_data = rdd_data.map(lambda x: x.split(\"|\")).filter(lambda x: len(x) == 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple total count operation with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken:  3.006709337234497\n"
     ]
    }
   ],
   "source": [
    "# time to count\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "total_count = rdd_data.count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the same operation on Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_list = [StructField('name',StringType(),True),\n",
    "              StructField('city',StringType(),True),\n",
    "              StructField('state',StringType(),True),\n",
    "              StructField('category',StringType(),True),\n",
    "              StructField('score',FloatType(),False),\n",
    "              StructField('amount',FloatType(),False),]\n",
    "\n",
    "my_schema = StructType(schema_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spark.read.option(\"delimiter\",\"|\").csv(\"<csv-path>\",schema=my_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple count operation on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken:  0.7043869495391846\n"
     ]
    }
   ],
   "source": [
    "# time to count\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "total_count = data_frame.count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do DataFrames perform better?\n",
    "\n",
    "<p> As network latency has decreased the new compact representation of DataFrames gives<br>\n",
    "it an edge over RDDs<p>\n",
    "\n",
    "![alt text](./img/hw_trend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some more complex queries and compare\n",
    "\n",
    "- total sale\n",
    "- total sale per state\n",
    "- unique categories\n",
    "and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken by RDD:  1.6312265396118164\n",
      "Total time taken by DF:  1.0419425964355469\n"
     ]
    }
   ],
   "source": [
    "# Total Sale\n",
    "start_time = time.time()\n",
    "total_sum = rdd_data.map(lambda x: float(x[-1])).sum()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by RDD: \",end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "total_sum = data_frame.agg(F.sum(\"amount\")).collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by DF: \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken by RDD:  2.0678892135620117\n",
      "Total time taken by DF:  1.219912052154541\n"
     ]
    }
   ],
   "source": [
    "# Total Sale per State\n",
    "start_time = time.time()\n",
    "total_sum = rdd_data.map(lambda x: (x[2],float(x[-1]))).groupByKey().mapValues(sum).collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by RDD: \",end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "total_sum = data_frame.groupBy('state').agg(F.sum(\"amount\")).collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by DF: \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken by RDD:  1.8873486518859863\n",
      "Total time taken by DF:  1.5895500183105469\n"
     ]
    }
   ],
   "source": [
    "# All unique categories\n",
    "start_time = time.time()\n",
    "categories = rdd_data.map(lambda x: x[3]).distinct().collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by RDD: \",end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "categories = data_frame.select('category').distinct().collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by DF: \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/space-improvement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Dataset API:\n",
    "- Compact (less overhead)\n",
    "- Reduce our memory footprint significantly.\n",
    "- Spark knows what data is it handling now.\n",
    "- Spark also knows the operation that user wants to perform on Dataset\n",
    "- Both the above two listed benefits paved way to one more not so obvious advantage which is:\n",
    "- Possible in-place transformations for simple one’s without the need to deserialise. (Let’s see how this happens in detail below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we further improve our performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/parquet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_data = spark.read.parquet('<parquet_path>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+-----+---------+\n",
      "|                name|                city|state|            category|score|   amount|\n",
      "+--------------------+--------------------+-----+--------------------+-----+---------+\n",
      "|   St William Church|              Warren|   OH|Community and Gov...|  0.9| 78.09576|\n",
      "|Centre Elementary...|              Centre|   AL|Community and Gov...|  1.0|54.818233|\n",
      "|         Mobile Edge|           Lehighton|   PA|Automotive,Mainte...|  0.9| 9.512629|\n",
      "|National Park Com...|Hot Springs Natio...|   AR|Community and Gov...|  1.0|34.694485|\n",
      "|Mr G Natural Heal...|        Mount Vernon|   NY|Retail,Food and B...|  0.9| 63.44141|\n",
      "+--------------------+--------------------+-----+--------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken by RDD:  2.0981931686401367\n",
      "Total time taken by DF:  0.7468547821044922\n"
     ]
    }
   ],
   "source": [
    "# Total Sale per State\n",
    "start_time = time.time()\n",
    "total_sum = rdd_data.map(lambda x: (x[2],float(x[-1]))).groupByKey().mapValues(sum).collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by RDD: \",end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "total_sum = parquet_data.groupBy('state').agg(F.sum(\"amount\")).collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time taken by DF: \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
