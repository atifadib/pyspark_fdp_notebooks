{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to configure your Spark Application\n",
    "\n",
    "#### spark-submit --class <CLASS_NAME> --num-executors ? --executor-cores ? --executor-memory ? ....\n",
    "![alt text](./img/spark-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Classes to launch a Spark Context\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a configuration object\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some commonly used configuration properties?\n",
    "\n",
    "* Spark Master\n",
    "* Spark Application name\n",
    "* spark.driver.memory\n",
    "* spark.executor.memory\n",
    "* spark.executor.cores\n",
    "* spark.executor.instances\n",
    "* spark.driver.maxResultSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x11b41ad30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.setMaster('local[*]') # set to YARN if running on a cluster\n",
    "conf.setAppName('My Test Application')\n",
    "conf.set('spark.driver.memory', '4g')\n",
    "conf.set('spark.executor.memory', '2g')\n",
    "conf.set('spark.executor.cores', '7')\n",
    "conf.set('spark.executor.instances', '2')\n",
    "conf.set('spark.driver.maxResultSize', '12g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### What is Driver Memory?\n",
    "\n",
    "<p>When we start our spark application with spark submit command,\n",
    "a driver will start and that driver will contact spark master\n",
    "to launch executors and run the tasks.\n",
    "Basically, Driver is a representative of our application and does all the communication with Spark.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Executor Memory?\n",
    "\n",
    "<p> The amount of memory to be allocated to an executor, spark adds an addition overhead\n",
    "to the amount of memory which is 7% of requested memory before allocating the space </p>\n",
    "#### spark.executor.memory = requested_memory + max(384MB, 0.07*requested_memory)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a cluster\n",
    "\n",
    "<p> Let's assume our cluster configuration is as follows:<br>\n",
    "Total Nodes: <b>10</b> <br>\n",
    "Total Cores per Node: <b>16</b><br>\n",
    "Total Memory per Node: <b>64GB</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fat Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x11b4bd8d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration with fat executor\n",
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory', '48gb')\n",
    "conf.set('spark.executor.memory','58gb') # Not 64gb, because we need some memory for executor overhead\n",
    "conf.set('spark.executor.cores', '15') # Not 16 cores, leaving one core for daemons to run smoothly\n",
    "conf.set('spark.executor.instances', '9') # One executor per node, except on master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> This configuration launches 9 instances of executors, <br>\n",
    "each taking up one node in the cluster </p><br>\n",
    "<p> Lauching a Spark Job with this configuration is a bad idea,<br>\n",
    "as most application read and write data from a distributed file system<br>\n",
    "like HDFS, S3 or GFS. Such a configuration will lead to a lot of <br>\n",
    "garbage collection and reduced parallelism </p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lean Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x11b4c3400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration with lean executor\n",
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory', '48gb')\n",
    "conf.set('spark.executor.memory','3gb') # why not 4gb?\n",
    "conf.set('spark.executor.cores', '1') \n",
    "conf.set('spark.executor.instances', '135') # why not 144?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In this configuration, there is an executor running on each core<br>\n",
    "In total we have 10*16 cores = 160, but 10 of them are on driver.<br>\n",
    "So, 150 cores, can we launch 150 executors??</p>\n",
    "\n",
    "<p> Out of the 64gb memory on each node, so if 16 executors are running,<br>\n",
    "can we can give each executor 64/16 = 4gb memory? </p>\n",
    "<br>\n",
    "\n",
    "<p> With a lean configuration containing many executors, <br>\n",
    "we'd not be able to take advantage of runnning multiple tasks within<br>\n",
    "the same JVM and cached/broadcasted variables will be replicated.<br>\n",
    "</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the best configuration?\n",
    "\n",
    "<p> The best configuration depends a lot on the type of<br>\n",
    "spark application you are running.</p>\n",
    "<br>\n",
    "<p> However, its a good idea to have atleast 5 cores per executor,<br>\n",
    "and choose executor memory accordingly.</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x11b4acc18>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.driver.memory', '48gb')\n",
    "conf.set('spark.executor.memory','10gb') \n",
    "conf.set('spark.executor.cores', '5') \n",
    "conf.set('spark.executor.instances', '30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/aws-emr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Allocation in Spark\n",
    "\n",
    "#### spark.dynamicAllocation.enabled\n",
    "<p>It allows spark to spawn and kill executors dynamically based on the application,<br>\n",
    "However, you still need to provide some initial condition as well as bounds for<br>\n",
    "number of executors.<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.set('spark.dynamicAllocation.enabled', 'true')\n",
    "conf.set(\"spark.dynamicAllocation.minExecutors\", \"2\");\n",
    "conf.set(\"spark.dynamicAllocation.maxExecutors\", \"10\");\n",
    "conf.set(\"spark.dynamicAllocation.initialExecutors\", \"10\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Spark using Zeppelin\n",
    "\n",
    "<p>If you are running Spark on a web-based notebook service like Zeppelin,<br>\n",
    "SparkContext is already initialised, and to change the configuration<br>\n",
    "you have to use sc._conf object.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./img/zeppelin.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1592161710152'),\n",
       " ('spark.executor.memory', '2g'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.app.name', 'My Test Application'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.dynamicAllocation.minExecutors', '2'),\n",
       " ('spark.driver.maxResultSize', '12g'),\n",
       " ('spark.driver.port', '50680'),\n",
       " ('spark.driver.host', '192.168.0.101'),\n",
       " ('spark.executor.cores', '7'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.dynamicAllocation.initialExecutors', '10'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting a Spark SQL Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
